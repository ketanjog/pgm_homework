{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbf4cc7",
   "metadata": {},
   "source": [
    "# Building out Bayesian Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd8df4f",
   "metadata": {},
   "source": [
    "#### We assume that the imdb reviews dataset (review, sentiment) has been tokenised using TF-idf. We further assume that truncated SVD has been applied to the data to reduce the number of features to 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beecab1a",
   "metadata": {},
   "source": [
    "#### 100 dimensions is default for now but its easy to change this. Just change REQD_DIMS variable below to whatever your data has. Note that large number of features, especially like ones generated by TF-idf will be expensive and possibly lead to bad predictive power)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace71f7f",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97733e2e",
   "metadata": {},
   "source": [
    "# Set up the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6767d4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ketanjog/Documents/Columbia_Classes/Year_5/Semester_9/pgm/homework/homework_01/venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ketanjog/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "UsageError: Line magic function `%matplotlib.inline` not found.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pgm.utils.math import sigmoid\n",
    "from pgm.data.load_imdb import load_processed_imdb_data\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib.inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54755cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running BoW the dataset and reducing dimensionality with min_df %d 100\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-------------------------\n",
      "Setting sentiment + == 1 ; - == 0\n"
     ]
    }
   ],
   "source": [
    "features, responses = load_processed_imdb_data(dims=100, load=True, representation=\"bow\", min_df=100)\n",
    "# BoW count vectoriser - min_df~10 min amount of timesthe word show up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcbcce50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLogisticRegression:\n",
    "    def __init__(self, prior_variance: float, learning_rate: float = 0.01):\n",
    "        \"\"\"\n",
    "        Initialisation parameters:\n",
    "\n",
    "        prior_variance : float = the variance on the Bernoulli prior for the betas\n",
    "        learning_rate : float = the step size for stochastic gradient ascent\n",
    "\n",
    "        \"\"\"\n",
    "        self.prior_variance = prior_variance\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta: np.ndarray = None\n",
    "\n",
    "        self.features: np.ndarray = None\n",
    "        self.responses: np.ndarray = None\n",
    "        self.term_1 = None\n",
    "\n",
    "        # Storing data for plotting\n",
    "        self.log_likelihood_values = []\n",
    "        self.accuracy_values = []\n",
    "        self.grad_values = []\n",
    "\n",
    "    def fit(self, features: np.ndarray, responses: np.ndarray, verbose=True):\n",
    "        \"\"\"\n",
    "        Fits the Bayesian Logistic model to the dataset\n",
    "        \"\"\"\n",
    "        # Load the features and responses into class variables\n",
    "        split = int(len(responses) * 0.8)\n",
    "\n",
    "        # Shuffle and choose random subset of data\n",
    "        new_index = np.random.permutation(len(responses))\n",
    "\n",
    "        # Training set\n",
    "        self.train_features = features[new_index][:split]\n",
    "        self.train_responses = responses[new_index][:split]\n",
    "\n",
    "        # Testing set\n",
    "        self.test_features = features[new_index][split:]\n",
    "        self.test_responses = responses[new_index][split:]\n",
    "\n",
    "        self.beta = np.zeros([features.shape[1]])\n",
    "        lg = Logger(verbose=verbose)\n",
    "\n",
    "        # assert self.features.shape == (50001, 100)\n",
    "\n",
    "        # run stochastic gradient ascent on the log joint\n",
    "        lg.print(\"Running stochastic gradient ascent\")\n",
    "        self.stochastic_gradient_ascent(verbose=verbose)\n",
    "        lg.print(\"Fit Successful!\")\n",
    "\n",
    "        # return self.beta\n",
    "\n",
    "    def predict(self, data_train: np.ndarray):\n",
    "        \"\"\"\n",
    "        Returns E[response | new_features, beta_fitted]\n",
    "        \"\"\"\n",
    "        if self.train_features is None or self.train_responses is None:\n",
    "            raise ValueError(\"Fit the model on data first\")\n",
    "        response = sigmoid(np.matmul(data_train, self.beta))\n",
    "        response[response > 0.5] = 1\n",
    "        response[response <= 0.5] = 0\n",
    "\n",
    "        return np.asarray(response).reshape(-1)\n",
    "\n",
    "    def objective(self):\n",
    "        \"\"\"\n",
    "        Return log of the joint\n",
    "        \"\"\"\n",
    "        if self.features is None or self.responses is None:\n",
    "            raise ValueError(\"Fit the model on data first\")\n",
    "\n",
    "        term_1 = np.matmul(self.features, self.beta)\n",
    "        self.term_1 = term_1\n",
    "\n",
    "        return (\n",
    "            np.dot(np.log(sigmoid((term_1))).T, self.responses)\n",
    "            + np.dot(np.log(sigmoid((-term_1))).T, (1 - self.responses))\n",
    "            - self.prior_variance * np.sum(self.beta**2)\n",
    "        )\n",
    "        # test with one objective\n",
    "\n",
    "    def grad_objective(self, batch: int):\n",
    "        \"\"\"\n",
    "        returns the gradient of the objective using the current beta\n",
    "        given the features and response data\n",
    "        \"\"\"\n",
    "        if self.features is None or self.responses is None:\n",
    "            raise ValueError(\"Fit the model on data first\")\n",
    "\n",
    "        deviation = (\n",
    "            np.dot(self.features.T, (self.responses - sigmoid(self.term_1))) / batch\n",
    "        )\n",
    "        regulariser = self.prior_variance * self.beta\n",
    "        gradient = -regulariser + deviation\n",
    "\n",
    "        # assert gradient.shape[0] == self.beta.shape[0]\n",
    "\n",
    "        return gradient\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.y_hat = self.predict(self.test_features)\n",
    "        self.test_size = len(self.test_responses)\n",
    "\n",
    "        y_hat = self.predict(self.test_features)\n",
    "        test_size = len(self.test_responses)\n",
    "\n",
    "        # print(str(np.sum(np.absolute(y_hat - self.test_responses))))\n",
    "        accuracy = (\n",
    "            test_size - np.sum(np.absolute(y_hat - self.test_responses))\n",
    "        ) / test_size\n",
    "\n",
    "        return accuracy\n",
    "\n",
    "    def stochastic_gradient_ascent(self, batch=100, smart_stopping=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Performs stochastic gradient ascent on the data\n",
    "        to fit beta\n",
    "        \"\"\"\n",
    "        lg = Logger(verbose=verbose)\n",
    "        if self.train_features is None or self.train_responses is None:\n",
    "            raise ValueError(\"Fit the model on data first\")\n",
    "\n",
    "        # Initialise beta to be zeros\n",
    "        self.beta = np.zeros_like(self.beta)\n",
    "        self.beta = self.beta.reshape((len(self.beta), 1))\n",
    "\n",
    "        # Iterating until convergence\n",
    "        if smart_stopping:\n",
    "\n",
    "            iterations = 0\n",
    "            MAX_ITERATIONS = 10000\n",
    "\n",
    "            while True:\n",
    "\n",
    "                # Shuffle and choose random subset of data\n",
    "                # new_index = np.random.permutation(batch)\n",
    "                \n",
    "                self.features = self.train_features[iterations*batch:(iterations+1)*batch]\n",
    "                self.responses = self.train_responses[iterations*batch:(iterations+1)*batch].reshape(-1, 1)\n",
    "\n",
    "                # Save old likelihood\n",
    "                new_likelihood = self.objective()\n",
    "\n",
    "                old_likelihood = (\n",
    "                    0\n",
    "                    if len(self.log_likelihood_values) == 0\n",
    "                    else self.log_likelihood_values[-1]\n",
    "                )\n",
    "\n",
    "                # Check for convergence\n",
    "                if abs(new_likelihood - old_likelihood) < 0.0001:\n",
    "                    lg.print_progress(iterations, frequency=1)\n",
    "                    break\n",
    "\n",
    "                # Save new likelihood value\n",
    "                self.log_likelihood_values.append(new_likelihood)\n",
    "                self.accuracy_values.append(self.evaluate())\n",
    "                self.grad_values.append(self.grad_objective(batch))\n",
    "\n",
    "                # Update beta\n",
    "                grad = self.grad_objective(batch=batch)\n",
    "\n",
    "                self.beta += np.multiply(self.learning_rate, grad)\n",
    "                iterations += 1\n",
    "\n",
    "                # Print progress nicely\n",
    "                # lg.print_progress(iterations)\n",
    "                if iterations % 10 == 0:\n",
    "                    print(\n",
    "                        f\" Log Likelihood is{self.log_likelihood_values[-1].item():.2f} \"\n",
    "                    )\n",
    "                    print(f\" Accuracy is{self.accuracy_values[-1]:.2f} \")\n",
    "\n",
    "                # For testing, if stop if we exceed max iterations\n",
    "                if iterations > MAX_ITERATIONS:\n",
    "                    print(\"Reached maximum iterations. Breaking out of optimisation\")\n",
    "                    break\n",
    "\n",
    "    def plot_likelihood(self):\n",
    "        plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "        plt.title(\"Likelihood\")\n",
    "        plt.plot(np.asarray(self.log_likelihood_values).reshape(-1), color=\"red\")\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    def plot_accuracy(self):\n",
    "        plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "        plt.rcParams[\"figure.autolayout\"] = True\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.plot(self.accuracy_values, color=\"red\")\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27b093c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running stochastic gradient ascent\n",
      " Log Likelihood is-68.83 \n",
      " Accuracy is0.64 \n",
      " Log Likelihood is-68.44 \n",
      " Accuracy is0.68 \n",
      " Log Likelihood is-68.29 \n",
      " Accuracy is0.72 \n",
      " Log Likelihood is-68.89 \n",
      " Accuracy is0.68 \n",
      " Log Likelihood is-68.14 \n",
      " Accuracy is0.63 \n",
      " Log Likelihood is-68.58 \n",
      " Accuracy is0.61 \n",
      " Log Likelihood is-69.17 \n",
      " Accuracy is0.61 \n",
      " Log Likelihood is-68.72 \n",
      " Accuracy is0.69 \n",
      " Log Likelihood is-68.47 \n",
      " Accuracy is0.71 \n",
      " Log Likelihood is-68.46 \n",
      " Accuracy is0.71 \n",
      " Log Likelihood is-69.05 \n",
      " Accuracy is0.60 \n",
      " Log Likelihood is-68.32 \n",
      " Accuracy is0.66 \n",
      " Log Likelihood is-68.39 \n",
      " Accuracy is0.62 \n",
      " Log Likelihood is-68.93 \n",
      " Accuracy is0.60 \n",
      " Log Likelihood is-68.47 \n",
      " Accuracy is0.66 \n",
      " Log Likelihood is-67.91 \n",
      " Accuracy is0.59 \n",
      " Log Likelihood is-68.35 \n",
      " Accuracy is0.65 \n",
      " Log Likelihood is-68.16 \n",
      " Accuracy is0.61 \n",
      " Log Likelihood is-68.70 \n",
      " Accuracy is0.62 \n",
      " Log Likelihood is-68.80 \n",
      " Accuracy is0.69 \n",
      " Log Likelihood is-68.73 \n",
      " Accuracy is0.65 \n",
      " Log Likelihood is-68.40 \n",
      " Accuracy is0.64 \n",
      " Log Likelihood is-68.34 \n",
      " Accuracy is0.68 \n",
      " Log Likelihood is-68.33 \n",
      " Accuracy is0.60 \n",
      " Log Likelihood is-68.16 \n",
      " Accuracy is0.59 \n",
      " Log Likelihood is-68.55 \n",
      " Accuracy is0.66 \n",
      " Log Likelihood is-68.58 \n",
      " Accuracy is0.68 \n",
      " Log Likelihood is-68.62 \n",
      " Accuracy is0.68 \n",
      " Log Likelihood is-68.61 \n",
      " Accuracy is0.64 \n",
      " Log Likelihood is-68.15 \n",
      " Accuracy is0.65 \n",
      " Log Likelihood is-68.25 \n",
      " Accuracy is0.65 \n",
      " Log Likelihood is-68.78 \n",
      " Accuracy is0.64 \n",
      " Log Likelihood is-68.65 \n",
      " Accuracy is0.61 \n",
      " Log Likelihood is-68.65 \n",
      " Accuracy is0.68 \n",
      " Log Likelihood is-68.41 \n",
      " Accuracy is0.70 \n",
      " Log Likelihood is-68.98 \n",
      " Accuracy is0.64 \n",
      " Log Likelihood is-68.91 \n",
      " Accuracy is0.56 \n",
      " Log Likelihood is-68.56 \n",
      " Accuracy is0.67 \n",
      " Log Likelihood is-68.71 \n",
      " Accuracy is0.70 \n",
      " Log Likelihood is-68.49 \n",
      " Accuracy is0.67 \n",
      " Log Likelihood is-0.00 \n",
      " Accuracy is0.62 \n",
      "---------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Logger' object has no attribute 'log_likelihood_values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m blr \u001b[38;5;241m=\u001b[39m BayesianLogisticRegression(prior_variance\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m blr\u001b[38;5;241m.\u001b[39mfit(features\u001b[38;5;241m=\u001b[39mfeatures, responses\u001b[38;5;241m=\u001b[39mresponses, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn [17], line 48\u001b[0m, in \u001b[0;36mBayesianLogisticRegression.fit\u001b[0;34m(self, features, responses, verbose)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# assert self.features.shape == (50001, 100)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# run stochastic gradient ascent on the log joint\u001b[39;00m\n\u001b[1;32m     47\u001b[0m lg\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning stochastic gradient ascent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstochastic_gradient_ascent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m lg\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFit Successful!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn [17], line 152\u001b[0m, in \u001b[0;36mBayesianLogisticRegression.stochastic_gradient_ascent\u001b[0;34m(self, batch, smart_stopping, verbose)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# Check for convergence\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mabs\u001b[39m(new_likelihood \u001b[38;5;241m-\u001b[39m old_likelihood) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0001\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[43mlg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_progress\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;66;03m# Save new likelihood value\u001b[39;00m\n",
      "Cell \u001b[0;32mIn [11], line 16\u001b[0m, in \u001b[0;36mLogger.print_progress\u001b[0;34m(self, iteration, frequency)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m---------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe Log Likelihood is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_likelihood_values[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     percent_increase \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\n\u001b[1;32m     18\u001b[0m         (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_likelihood_values[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_likelihood_values[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_likelihood_values[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     20\u001b[0m     )\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLikelihood increased by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpercent_increase\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m percent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Logger' object has no attribute 'log_likelihood_values'"
     ]
    }
   ],
   "source": [
    "blr = BayesianLogisticRegression(prior_variance=10, learning_rate=0.01)\n",
    "blr.fit(features=features, responses=responses, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938db55b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blr.plot_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a74375",
   "metadata": {},
   "outputs": [],
   "source": [
    "blr.plot_likelihood()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da223bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.title(\"Gradients\")\n",
    "plt.plot(np.asarray(blr.grad_values), color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020a7e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(blr.grad_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049a97b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# held out predictive log likelihood for hyperparameter tuning\n",
    "# Or grid search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38598e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.title(\"Gradients\")\n",
    "plt.plot(blr.grad_values, color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffe3351",
   "metadata": {},
   "outputs": [],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9776550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "blr.responses.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbf603",
   "metadata": {},
   "outputs": [],
   "source": [
    "blr.responses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b88ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blr.beta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfdcb7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "blr.beta.reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1150193",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = blr.grad_objective().reshape(-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077e0882",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ab0ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b759eee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "blr.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "blr.test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc16b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "blr.y_hat.T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b8f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67068b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.asarray(blr.y_hat).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e848c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\n",
    "plt.rcParams[\"figure.autolayout\"] = True\n",
    "plt.title(\"Likelihood\")\n",
    "plt.plot(np.asarray(blr.log_likelihood_values).reshape(-1), color=\"red\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585da33",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(blr.log_likelihood_values).reshape(-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
